
@misc{ducoffe_lard_2023,
	title = {{LARD} -- {Landing} {Approach} {Runway} {Detection} -- {Dataset} for {Vision} {Based} {Landing}},
	url = {http://arxiv.org/abs/2304.09938},
	doi = {10.48550/arXiv.2304.09938},
	abstract = {As the interest in autonomous systems continues to grow, one of the major challenges is collecting sufficient and representative real-world data. Despite the strong practical and commercial interest in autonomous landing systems in the aerospace field, there is a lack of open-source datasets of aerial images. To address this issue, we present a dataset-lard-of high-quality aerial images for the task of runway detection during approach and landing phases. Most of the dataset is composed of synthetic images but we also provide manually labelled images from real landing footages, to extend the detection task to a more realistic setting. In addition, we offer the generator which can produce such synthetic front-view images and enables automatic annotation of the runway corners through geometric transformations. This dataset paves the way for further research such as the analysis of dataset quality or the development of models to cope with the detection tasks. Find data, code and more up-to-date information at https://github.com/deel-ai/LARD},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Ducoffe, Mélanie and Carrere, Maxime and Féliers, Léo and Gauffriau, Adrien and Mussot, Vincent and Pagetti, Claire and Sammour, Thierry},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09938 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/QN2Q2GGR/Ducoffe et al. - 2023 - LARD -- Landing Approach Runway Detection -- Dataset for Vision Based Landing.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/Y8IPD5NS/2304.html:text/html},
}

@misc{chen_bars_2023,
	title = {{BARS}: {A} {Benchmark} for {Airport} {Runway} {Segmentation}},
	shorttitle = {{BARS}},
	url = {http://arxiv.org/abs/2210.12922},
	doi = {10.48550/arXiv.2210.12922},
	abstract = {Airport runway segmentation can effectively reduce the accident rate during the landing phase, which has the largest risk of flight accidents. With the rapid development of deep learning (DL), related methods achieve good performance on segmentation tasks and can be well adapted to complex scenes. However, the lack of large-scale, publicly available datasets in this field makes the development of methods based on DL difficult. Therefore, we propose a benchmark for airport runway segmentation, named BARS. Additionally, a semiautomatic annotation pipeline is designed to reduce the annotation workload. BARS has the largest dataset with the richest categories and the only instance annotation in the field. The dataset, which was collected using the X-Plane simulation platform, contains 10,256 images and 30,201 instances with three categories. We evaluate eleven representative instance segmentation methods on BARS and analyze their performance. Based on the characteristic of an airport runway with a regular shape, we propose a plug-and-play smoothing postprocessing module (SPM) and a contour point constraint loss (CPCL) function to smooth segmentation results for mask-based and contour-based methods, respectively. Furthermore, a novel evaluation metric named average smoothness (AS) is developed to measure smoothness. The experiments show that existing instance segmentation methods can achieve prediction results with good performance on BARS. SPM and CPCL can effectively enhance the AS metric while modestly improving accuracy. Our work will be available at https://github.com/c-wenhui/BARS.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Chen, Wenhui and Zhang, Zhijiang and Yu, Liang and Tai, Yichun},
	month = apr,
	year = {2023},
	note = {arXiv:2210.12922 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Applied Intelligence 2023},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/B773FYBB/Chen et al. - 2023 - BARS A Benchmark for Airport Runway Segmentation.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/7SHK7QAK/2210.html:text/html},
}

@article{wang_valnet_2024,
	title = {{VALNet}: {Vision}-{Based} {Autonomous} {Landing} with {Airport} {Runway} {Instance} {Segmentation}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {{VALNet}},
	url = {https://www.mdpi.com/2072-4292/16/12/2161},
	doi = {10.3390/rs16122161},
	abstract = {Visual navigation, characterized by its autonomous capabilities, cost effectiveness, and robust resistance to interference, serves as the foundation for vision-based autonomous landing systems. These systems rely heavily on runway instance segmentation, which accurately divides runway areas and provides precise information for unmanned aerial vehicle (UAV) navigation. However, current research primarily focuses on runway detection but lacks relevant runway instance segmentation datasets. To address this research gap, we created the Runway Landing Dataset (RLD), a benchmark dataset that focuses on runway instance segmentation mainly based on X-Plane. To overcome the challenges of large-scale changes and input image angle differences in runway instance segmentation tasks, we propose a vision-based autonomous landing segmentation network (VALNet) that uses band-pass filters, where a Context Enhancement Module (CEM) guides the model to learn adaptive “band” information through heatmaps, while an Orientation Adaptation Module (OAM) of a triple-channel architecture to fully utilize rotation information enhances the model’s ability to capture input image rotation transformations. Extensive experiments on RLD demonstrate that the new method has significantly improved performance. The visualization results further confirm the effectiveness and interpretability of VALNet in the face of large-scale changes and angle differences. This research not only advances the development of runway instance segmentation but also highlights the potential application value of VALNet in vision-based autonomous landing systems. Additionally, RLD is publicly available.},
	language = {en},
	number = {12},
	urldate = {2024-12-14},
	journal = {Remote Sensing},
	author = {Wang, Qiang and Feng, Wenquan and Zhao, Hongbo and Liu, Binghao and Lyu, Shuchang},
	month = jan,
	year = {2024},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {band-pass filtering, heatmap guided, instance segmentation, vision-based autonomous landing},
	pages = {2161},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/4ZIUGYLA/Wang et al. - 2024 - VALNet Vision-Based Autonomous Landing with Airport Runway Instance Segmentation.pdf:application/pdf},
}

@inproceedings{akbar_runway_2019,
	title = {Runway {Detection} and {Localization} in {Aerial} {Images} using {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8945889},
	doi = {10.1109/DICTA47822.2019.8945889},
	abstract = {Landing is the most difficult phase of the flight for any airborne platform. Due to lack of efficient systems, there have been numerous landing accidents resulting in the damage of onboard hardware. Vision based systems provides low cost solution to detect landing sites by providing rich textual information. To this end, this research focuses on accurate detection and localization of runways in aerial images with untidy terrains which would consequently help aerial platforms especially Unmanned Aerial Vehicles (commonly referred to as Drones) to detect landing targets (i.e., runways) to aid automatic landing. Most of the prior work regarding runway detection is based on simple image processing algorithms with lot of assumptions and constraints about precise position of runway in a particular image. First part of this research is to develop runway detection algorithm based on state-of-the-art deep learning architectures while the second part is runway localization using both deep learning and non-deep learning based methods. The proposed runway detection approach is two-stage modular where in the first stage the aerial image classification is achieved to find the existence of runway in that particular image. Later, in the second stage, the identified runways are localized using both conventional line detection algorithms and more recent deep learning models. The runway classification has been achieved with an accuracy of around 97\% whereas the runways have been localized with mean Intersection-over-Union (IoU) score of 0.8.},
	urldate = {2024-12-14},
	booktitle = {2019 {Digital} {Image} {Computing}: {Techniques} and {Applications} ({DICTA})},
	author = {Akbar, Javeria and Shahzad, Muhammad and Malik, Muhammad Imran and Ul-Hasan, Adnan and Shafait, Fasial},
	month = dec,
	year = {2019},
	keywords = {Airports, deep learning, Deep learning, detection, Feature extraction, Image edge detection, localization, runway, Training, Transforms},
	pages = {1--8},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/4DJKFW2V/Akbar et al. - 2019 - Runway Detection and Localization in Aerial Images using Deep Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/gustavo/Zotero/storage/P3TJEMES/8945889.html:text/html},
}

@misc{yang_diffusion_2024,
	title = {Diffusion {Models}: {A} {Comprehensive} {Survey} of {Methods} and {Applications}},
	shorttitle = {Diffusion {Models}},
	url = {http://arxiv.org/abs/2209.00796},
	doi = {10.48550/arXiv.2209.00796},
	abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
	month = dec,
	year = {2024},
	note = {arXiv:2209.00796 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 58 pages, 19 figures, citing 389 (up-to-date) papers, project: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy, accepted by ACM Computing Surveys},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/SDWEEU8J/Yang et al. - 2024 - Diffusion Models A Comprehensive Survey of Methods and Applications.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/TULNG75V/2209.html:text/html},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2024-12-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/3RW6MBTY/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: conditionally accepted at MICCAI 2015},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/ZCWKYC5G/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/YA3IB4YG/1505.html:text/html},
}

@article{xin_vision-based_2022,
	title = {Vision-{Based} {Autonomous} {Landing} for the {UAV}: {A} {Review}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2226-4310},
	shorttitle = {Vision-{Based} {Autonomous} {Landing} for the {UAV}},
	url = {https://www.mdpi.com/2226-4310/9/11/634},
	doi = {10.3390/aerospace9110634},
	abstract = {With the rapid development of the UAV, it is widely used in rescue and disaster relief, where autonomous landing is the key technology. Vision-based autonomous landing has the advantages of strong autonomy, low cost, and strong anti-interference ability. Moreover, vision navigation has higher guidance and positioning accuracy combined with other navigation methods, such as GPS/INS. This paper summarizes the research results in the field of vision-based autonomous landing for the UAV, and divides it into static, dynamic, and complex scenarios according to the type of landing destination. Among them, the static scenario includes two categories: cooperative targets and natural landmarks; the dynamic scenario is divided into two categories: vehicle-based autonomous landing and ship-based autonomous landing. The key technologies are summarized, compared, and analyzed and the future development trends are pointed out, which can provide a reference for the research on vision-based autonomous landing of UAVs.},
	language = {en},
	number = {11},
	urldate = {2025-02-27},
	journal = {Aerospace},
	author = {Xin, Long and Tang, Zimu and Gai, Weiqi and Liu, Haobo},
	month = nov,
	year = {2022},
	keywords = {autonomous landing, computer vision, UAV},
	pages = {634},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/BCSX6AZR/Xin et al. - 2022 - Vision-Based Autonomous Landing for the UAV A Rev.pdf:application/pdf},
}

@article{li_factors_2001,
	title = {Factors associated with pilot error in aviation crashes},
	volume = {72},
	journal = {Aviation, space, and environmental medicine},
	author = {Li, Guohua and Baker, S and Grabowski, Jurek and Rebok, George},
	month = feb,
	year = {2001},
	pages = {52--8},
}

@misc{airbus_accidents_nodate,
	title = {Accidents by {Flight} {Phase} – accidentstats.airbus.com},
	url = {https://accidentstats.airbus.com/accidents-by-flight-phase/},
	language = {en-US},
	urldate = {2025-02-27},
	author = {Airbus},
}

@misc{airbus_fatal_nodate,
	title = {Fatal {Accidents} – accidentstats.airbus.com},
	url = {https://accidentstats.airbus.com/fatal-accidents/},
	language = {en-US},
	urldate = {2025-02-27},
	author = {Airbus},
}

@misc{boeing_statistical_2024,
	title = {Statistical {Summary} of {Commercial} {Jet} {Airplane} {Accidents}},
	url = {https://www.boeing.com/content/dam/boeing/boeingdotcom/company/about_bca/pdf/statsum.pdf},
	urldate = {2025-03-03},
	publisher = {Boeing},
	author = {Boeing},
	month = aug,
	year = {2024},
	file = {PDF:/Users/gustavo/Zotero/storage/VDRQPZ43/statsum.pdf:application/pdf},
}

@misc{airbus_airbus_2021,
	title = {Airbus concludes {ATTOL} with fully autonomous flight tests {\textbar} {Airbus}},
	url = {https://www.airbus.com/en/newsroom/press-releases/2020-06-airbus-concludes-attol-with-fully-autonomous-flight-tests},
	abstract = {Following an extensive two-year flight test programme, Airbus has successfully concluded its Autonomous Taxi, Take-Off and Landing (ATTOL) project.},
	language = {en},
	urldate = {2025-03-03},
	author = {Airbus},
	month = oct,
	year = {2021},
	note = {Section: Innovation},
	file = {Snapshot:/Users/gustavo/Zotero/storage/7GVQKR2R/2020-06-airbus-concludes-attol-with-fully-autonomous-flight-tests.html:text/html},
}

@article{aytekin_texture-based_2013,
	title = {Texture-{Based} {Airport} {Runway} {Detection}},
	volume = {10},
	issn = {1558-0571},
	url = {https://ieeexplore.ieee.org/abstract/document/6269052},
	doi = {10.1109/LGRS.2012.2210189},
	abstract = {The automatic detection of airports is essential due to the strategic importance of these targets. In this letter, a runway detection method based on textural properties is proposed since they are the most descriptive element of an airport. Since the best discriminative features for airport runways cannot be trivially predicted, the Adaboost algorithm is employed as a feature selector over a large set of features. Moreover, the selected features with corresponding weights can provide information on the hidden characteristics of runways. Thus, the Adaboost-based selected feature subset can be used for both detecting runways and identifying their textural characteristics. Thus, a coarse representation of possible runway locations is obtained. The performance of the proposed approach was validated by experiments carried on a data set of large images consisting of heavily negative samples.},
	number = {3},
	urldate = {2025-03-03},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Aytekin, Ö. and Zöngür, U. and Halici, U.},
	month = may,
	year = {2013},
	note = {Conference Name: IEEE Geoscience and Remote Sensing Letters},
	keywords = {Adaboost algorithm, airport runway detection, Airports, Feature extraction, Remote sensing, Roads, satellite images, Satellites, Support vector machines, textural features, Training},
	pages = {471--475},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/3Q4FHBKK/Aytekin et al. - 2013 - Texture-Based Airport Runway Detection.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/gustavo/Zotero/storage/YU2NUE2A/6269052.html:text/html},
}

@inproceedings{ye_research_2020,
	title = {Research on {Vision}-based {Autonomous} {Landing} of {Unmanned} {Aerial} {Vehicle}},
	url = {https://ieeexplore.ieee.org/abstract/document/9315584},
	doi = {10.1109/AUTEEE50969.2020.9315584},
	abstract = {In order to ensure that the Unmanned Aerial Vehicle (UAV) achieves a safe and accurate landing under special conditions such as GPS signal interference, an autonomous landing method based on computer vision is proposed. Firstly, the projection model of the airborne camera is established, and the camera's internal parameter matrix is obtained by camera calibration. Then, the “H” landing target pattern is designed to reduce the complexity of image processing, and the white backing disc is added to make the background easier to the segment. Then the pose estimation model is established and combined with the coordinate information of the feature points to complete the pose calculation. Finally, the actual flight data of the UAV is compared with the pose estimation data, and the maximum error of the position and attitude estimation values is 0. 52m and 6.22°, which can basically meet the landing requirements of the UAV, and the feasibility of the method is proved.},
	urldate = {2025-03-03},
	booktitle = {2020 {IEEE} 3rd {International} {Conference} on {Automation}, {Electronics} and {Electrical} {Engineering} ({AUTEEE})},
	author = {Ye, Run and Tao, Chao and Yan, Bin and Yang, Ting},
	month = nov,
	year = {2020},
	keywords = {autonomous landing, Cameras, corner detection, Feature extraction, Global navigation satellite system, pose estimation, Radio navigation, Shape, UAV, Unmanned aerial vehicles, vision, Visualization},
	pages = {348--354},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/EN57UUZG/Ye et al. - 2020 - Research on Vision-based Autonomous Landing of Unmanned Aerial Vehicle.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/gustavo/Zotero/storage/HZK8KZX2/9315584.html:text/html},
}

@article{chen_image-based_2024,
	title = {An image-based runway detection method for fixed-wing aircraft based on deep neural network},
	volume = {18},
	copyright = {© 2024 The Authors. IET Image Processing published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
	issn = {1751-9667},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.13087},
	doi = {10.1049/ipr2.13087},
	abstract = {Visual information is important in final approach and landing phases for an approaching aircraft, it presents supplementary source for navigation system, and provides backup guidance when radio navigation fails, or even supports a complete vision-based landing. Relative position and attitude can be solved from the runway features in the image. Traditional runway detection methods have high latency and low accuracy, which is unable to satisfy the requirements for a safe landing. This paper proposes a real-time runway detection model, efficient runway feature extractor (ERFE), based on deep convolutional neural network, generating semantic segmentation and feature lines output. In order to evaluate the model's effectiveness, a benchmark is proposed to calculate the actual error between predicted feature line and ground truth one. A novel runway dataset which is based on pictures from Microsoft Flight Simulator 2020 (FS2020), is also proposed in this paper to train and test the model. The dataset will be released at https://www.kaggle.com/datasets/relufrank/fs2020-runway-dataset. ERFE shows excellent performance in FS2020 dataset, it gives satisfactory results even for real runway images excluded from our dataset.},
	language = {en},
	number = {8},
	urldate = {2025-03-03},
	journal = {IET Image Processing},
	author = {Chen, Mingqiang and Hu, Yuzhou},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/ipr2.13087},
	keywords = {edge detection, image classification, image segmentation, visual databases},
	pages = {1939--1949},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/8XJRMAVR/Chen and Hu - 2024 - An image-based runway detection method for fixed-wing aircraft based on deep neural network.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/ELQBAGCQ/ipr2.html:text/html},
}

@misc{loth_blessing_2024,
	title = {Blessing or curse? {A} survey on the {Impact} of {Generative} {AI} on {Fake} {News}},
	shorttitle = {Blessing or curse?},
	url = {http://arxiv.org/abs/2404.03021},
	doi = {10.48550/arXiv.2404.03021},
	abstract = {Fake news significantly influence our society. They impact consumers, voters, and many other societal groups. While Fake News exist for a centuries, Generative AI brings fake news on a new level. It is now possible to automate the creation of masses of high-quality individually targeted Fake News. On the other end, Generative AI can also help detecting Fake News. Both fields are young but developing fast. This survey provides a comprehensive examination of the research and practical use of Generative AI for Fake News detection and creation in 2024. Following the Structured Literature Survey approach, the paper synthesizes current results in the following topic clusters 1) enabling technologies, 2) creation of Fake News, 3) case study social media as most relevant distribution channel, 4) detection of Fake News, and 5) deepfakes as upcoming technology. The article also identifies current challenges and open issues.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Loth, Alexander and Kappes, Martin and Pahl, Marc-Oliver},
	month = dec,
	year = {2024},
	note = {arXiv:2404.03021 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 16 pages, 2 figures. Submitted to ACM Transactions on Intelligent Systems and Technology (ACM TIST). Added references},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/4Q2K85GG/Loth et al. - 2024 - Blessing or curse A survey on the Impact of Generative AI on Fake News.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/X2XFKDXX/2404.html:text/html},
}

@misc{saragih_using_2024,
	title = {Using {Diffusion} {Models} to {Generate} {Synthetic} {Labelled} {Data} for {Medical} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/2310.16794},
	doi = {10.48550/arXiv.2310.16794},
	abstract = {Medical image analysis has become a prominent area where machine learning has been applied. However, high quality, publicly available data is limited either due to patient privacy laws or the time and cost required for experts to annotate images. In this retrospective study, we designed and evaluated a pipeline to generate synthetic labeled polyp images for augmenting medical image segmentation models with the aim of reducing this data scarcity. In particular, we trained diffusion models on the HyperKvasir dataset, comprising 1000 images of polyps in the human GI tract from 2008 to 2016. Qualitative expert review, Fr{\textbackslash}'echet Inception Distance (FID), and Multi-Scale Structural Similarity (MS-SSIM) were tested for evaluation. Additionally, various segmentation models were trained with the generated data and evaluated using Dice score and Intersection over Union. We found that our pipeline produced images more akin to real polyp images based on FID scores, and segmentation performance also showed improvements over GAN methods when trained entirely, or partially, with synthetic data, despite requiring less compute for training. Moreover, the improvement persists when tested on different datasets, showcasing the transferability of the generated images.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Saragih, Daniel and Hibi, Atsuhiro and Tyrrell, Pascal},
	month = may,
	year = {2024},
	note = {arXiv:2310.16794 [eess]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: 25 pages, 6 figures, 10 tables},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/YCQKYQGF/Saragih et al. - 2024 - Using Diffusion Models to Generate Synthetic Labelled Data for Medical Image Segmentation.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/8WR78X3D/2310.html:text/html},
}

@article{reutov_generating_2023,
	series = {12th {International} {Young} {Scientists} {Conference} in {Computational} {Science}, {YSC2023}},
	title = {Generating of synthetic datasets using diffusion models for solving computer vision tasks in urban applications},
	volume = {229},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050923020264},
	doi = {10.1016/j.procs.2023.12.036},
	abstract = {The paper illustrates the process of generating synthetic data using diffusion neural networks for solving popular computer vision tasks in urban applications: traffic vehicles detection and classification. The result of this study demonstrated new approach for synthetic datasets creation using modern generative neural networks. To elaborate this approach, experiments were performed to generate synthetic dataset to solve the above basic computer vision problem in urban applications using the most advanced diffusion neural network models (Kandinsky 2.2). Experiments were performed on these datasets to train deep neural networks to solve the object detection problem (YOLOv5). The results of testing the trained detectors on a specially selected validation dataset showed the potential viability of a synthetic dataset generation approach using diffusion neural network models. However, full-fledged use of this approach to generate synthetic datasets that can be used in training deep neural networks of computer vision in production is accompanied by some difficulties, namely high difference between domains of real data and generated data, high labor costs of tuning trained diffusion models to achieve the highest quality of generation, etc.},
	urldate = {2025-03-03},
	journal = {Procedia Computer Science},
	author = {Reutov, Ilya},
	month = jan,
	year = {2023},
	keywords = {computer vision, diffusion models, synthetic datasets, vehicles detection},
	pages = {335--344},
	file = {ScienceDirect Snapshot:/Users/gustavo/Zotero/storage/ZETNYGCL/S1877050923020264.html:text/html},
}

@misc{voetman_big_2023,
	title = {The {Big} {Data} {Myth}: {Using} {Diffusion} {Models} for {Dataset} {Generation} to {Train} {Deep} {Detection} {Models}},
	shorttitle = {The {Big} {Data} {Myth}},
	url = {http://arxiv.org/abs/2306.09762},
	doi = {10.48550/arXiv.2306.09762},
	abstract = {Despite the notable accomplishments of deep object detection models, a major challenge that persists is the requirement for extensive amounts of training data. The process of procuring such real-world data is a laborious undertaking, which has prompted researchers to explore new avenues of research, such as synthetic data generation techniques. This study presents a framework for the generation of synthetic datasets by fine-tuning pretrained stable diffusion models. The synthetic datasets are then manually annotated and employed for training various object detection models. These detectors are evaluated on a real-world test set of 331 images and compared against a baseline model that was trained on real-world images. The results of this study reveal that the object detection models trained on synthetic data perform similarly to the baseline model. In the context of apple detection in orchards, the average precision deviation with the baseline ranges from 0.09 to 0.12. This study illustrates the potential of synthetic data generation techniques as a viable alternative to the collection of extensive training data for the training of deep models.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Voetman, Roy and Aghaei, Maya and Dijkstra, Klaas},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09762 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/UIRI8HQ9/Voetman et al. - 2023 - The Big Data Myth Using Diffusion Models for Dataset Generation to Train Deep Detection Models.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/FDC3GZN9/2306.html:text/html},
}

@misc{arkhipkin_kandinsky_2024,
	title = {Kandinsky 3.0 {Technical} {Report}},
	url = {http://arxiv.org/abs/2312.03511},
	doi = {10.48550/arXiv.2312.03511},
	abstract = {We present Kandinsky 3.0, a large-scale text-to-image generation model based on latent diffusion, continuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher quality and realism of image generation. In this report we describe the architecture of the model, the data collection procedure, the training technique, and the production system for user interaction. We focus on the key components that, as we have identified as a result of a large number of experiments, had the most significant impact on improving the quality of our model compared to the others. We also describe extensions and applications of our model, including super resolution, inpainting, image editing, image-to-video generation, and a distilled version of Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the reverse process and 20 times faster without visual quality decrease. By side-by-side human preferences comparison, Kandinsky becomes better in text understanding and works better on specific domains. The code is available at https://github.com/ai-forever/Kandinsky-3},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Arkhipkin, Vladimir and Filatov, Andrei and Vasilev, Viacheslav and Maltseva, Anastasia and Azizov, Said and Pavlov, Igor and Agafonova, Julia and Kuznetsov, Andrey and Dimitrov, Denis},
	month = jun,
	year = {2024},
	note = {arXiv:2312.03511 [cs]
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
	annote = {Comment: Project page: https://ai-forever.github.io/Kandinsky-3},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/PIYYPIF9/Arkhipkin et al. - 2024 - Kandinsky 3.0 Technical Report.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/PFSMGG7W/2312.html:text/html},
}

@article{betker_improving_nodate,
	title = {Improving {Image} {Generation} with {Better} {Captions}},
	url = {https://cdn.openai.com/papers/dall-e-3.pdf},
	abstract = {We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions. Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.},
	language = {en},
	author = {Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and Manassra, Wesam and Dhariwal, Prafulla and Chu, Casey and Jiao, Yunxin and Ramesh, Aditya},
	file = {PDF:/Users/gustavo/Zotero/storage/ZFAJRILR/Betker et al. - Improving Image Generation with Better Captions.pdf:application/pdf},
}

@inproceedings{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} {With} {Latent} {Diffusion} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper},
	language = {en},
	urldate = {2025-03-03},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	year = {2022},
	pages = {10684--10695},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/RMCVF5BS/Rombach et al. - 2022 - High-Resolution Image Synthesis With Latent Diffusion Models.pdf:application/pdf},
}

@misc{midjourney_midjourney_nodate,
	title = {Midjourney},
	url = {https://www.midjourney.com/website},
	abstract = {An independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.},
	urldate = {2025-03-03},
	journal = {Midjourney},
	author = {Midjourney},
	file = {Snapshot:/Users/gustavo/Zotero/storage/WM23D7P5/home.html:text/html},
}

@article{cheng_remote_2017,
	title = {Remote {Sensing} {Image} {Scene} {Classification}: {Benchmark} and {State} of the {Art}},
	volume = {105},
	issn = {1558-2256},
	shorttitle = {Remote {Sensing} {Image} {Scene} {Classification}},
	url = {https://ieeexplore.ieee.org/document/7891544},
	doi = {10.1109/JPROC.2017.2675998},
	abstract = {Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various data sets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning data sets and methods for scene classification is still lacking. In addition, almost all existing data sets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale data set, termed “NWPU-RESISC45,” which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This data set contains 31 500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 1) is large-scale on the scene classes and the total image number; 2) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion; and 3) has high within-class diversity and between-class similarity. The creation of this data set will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed data set, and the results are reported as a useful baseline for future research.},
	number = {10},
	urldate = {2025-03-04},
	journal = {Proceedings of the IEEE},
	author = {Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
	month = oct,
	year = {2017},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Benchmark data set, Benchmark testing, Classification, deep learning, handcrafted features, Image analysis, Machine learning, Remote sensing, remote sensing image, Satellites, scene classification, Social network services, Spatial resolution, unsupervised feature learning, Unsupervised learning},
	pages = {1865--1883},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/V66LF22Z/Cheng et al. - 2017 - Remote Sensing Image Scene Classification Benchmark and State of the Art.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/gustavo/Zotero/storage/LAAU9GSV/7891544.html:text/html},
}

@misc{mit_labelme_nodate,
	title = {{LabelMe}. {The} {Open} annotation tool},
	url = {http://labelme.csail.mit.edu/Release3.0/},
	urldate = {2025-03-04},
	author = {MIT},
	file = {LabelMe. The Open annotation tool:/Users/gustavo/Zotero/storage/AR74U4YV/Release3.0.html:text/html},
}

@article{fama_efficient_1970,
	title = {Efficient {Capital} {Markets}: {A} {Review} of {Theory} and {Empirical} {Work}},
	volume = {25},
	issn = {0022-1082},
	shorttitle = {Efficient {Capital} {Markets}},
	url = {https://www.jstor.org/stable/2325486},
	doi = {10.2307/2325486},
	number = {2},
	urldate = {2025-03-04},
	journal = {The Journal of Finance},
	author = {Fama, Eugene F.},
	year = {1970},
	note = {Publisher: [American Finance Association, Wiley]},
	pages = {383--417},
	file = {JSTOR Full Text PDF:/Users/gustavo/Zotero/storage/YV3KLPVZ/Fama - 1970 - Efficient Capital Markets A Review of Theory and Empirical Work.pdf:application/pdf},
}

@article{li_yolo-rwy_2024,
	title = {{YOLO}-{RWY}: {A} {Novel} {Runway} {Detection} {Model} for {Vision}-{Based} {Autonomous} {Landing} of {Fixed}-{Wing} {Unmanned} {Aerial} {Vehicles}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-446X},
	shorttitle = {{YOLO}-{RWY}},
	url = {https://www.mdpi.com/2504-446X/8/10/571},
	doi = {10.3390/drones8100571},
	abstract = {In scenarios where global navigation satellite systems (GNSSs) and radio navigation systems are denied, vision-based autonomous landing (VAL) for fixed-wing unmanned aerial vehicles (UAVs) becomes essential. Accurate and real-time runway detection in VAL is vital for providing precise positional and orientational guidance. However, existing research faces significant challenges, including insufficient accuracy, inadequate real-time performance, poor robustness, and high susceptibility to disturbances. To address these challenges, this paper introduces a novel single-stage, anchor-free, and decoupled vision-based runway detection framework, referred to as YOLO-RWY. First, an enhanced data augmentation (EDA) module is incorporated to perform various augmentations, enriching image diversity, and introducing perturbations that improve generalization and safety. Second, a large separable kernel attention (LSKA) module is integrated into the backbone structure to provide a lightweight attention mechanism with a broad receptive field, enhancing feature representation. Third, the neck structure is reorganized as a bidirectional feature pyramid network (BiFPN) module with skip connections and attention allocation, enabling efficient multi-scale and across-stage feature fusion. Finally, the regression loss and task-aligned learning (TAL) assigner are optimized using efficient intersection over union (EIoU) to improve localization evaluation, resulting in faster and more accurate convergence. Comprehensive experiments demonstrate that YOLO-RWY achieves AP50:95 scores of 0.760, 0.611, and 0.413 on synthetic, real nominal, and real edge test sets of the landing approach runway detection (LARD) dataset, respectively. Deployment experiments on an edge device show that YOLO-RWY achieves an inference speed of 154.4 FPS under FP32 quantization with an image size of 640. The results indicate that the proposed YOLO-RWY model possesses strong generalization and real-time capabilities, enabling accurate runway detection in complex and challenging visual environments, and providing support for the onboard VAL systems of fixed-wing UAVs.},
	language = {en},
	number = {10},
	urldate = {2025-03-04},
	journal = {Drones},
	author = {Li, Ye and Xia, Yu and Zheng, Guangji and Guo, Xiaoyang and Li, Qingfeng},
	month = oct,
	year = {2024},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {fixed-wing unmanned aerial vehicle, navigation denial, runway detection, vision-based autonomous landing},
	pages = {571},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/2D8WVPFK/Li et al. - 2024 - YOLO-RWY A Novel Runway Detection Model for Vision-Based Autonomous Landing of Fixed-Wing Unmanned.pdf:application/pdf},
}

@misc{myers_modified_2024,
	title = {Modified {CycleGAN} for the synthesization of samples for wheat head segmentation},
	url = {http://arxiv.org/abs/2402.15135},
	doi = {10.48550/arXiv.2402.15135},
	abstract = {Deep learning models have been used for a variety of image processing tasks. However, most of these models are developed through supervised learning approaches, which rely heavily on the availability of large-scale annotated datasets. Developing such datasets is tedious and expensive. In the absence of an annotated dataset, synthetic data can be used for model development; however, due to the substantial differences between simulated and real data, a phenomenon referred to as domain gap, the resulting models often underperform when applied to real data. In this research, we aim to address this challenge by first computationally simulating a large-scale annotated dataset and then using a generative adversarial network (GAN) to fill the gap between simulated and real images. This approach results in a synthetic dataset that can be effectively utilized to train a deep-learning model. Using this approach, we developed a realistic annotated synthetic dataset for wheat head segmentation. This dataset was then used to develop a deep-learning model for semantic segmentation. The resulting model achieved a Dice score of 83.4{\textbackslash}\% on an internal dataset and Dice scores of 79.6\% and 83.6\% on two external Global Wheat Head Detection datasets. While we proposed this approach in the context of wheat head segmentation, it can be generalized to other crop types or, more broadly, to images with dense, repeated patterns such as those found in cellular imagery.},
	urldate = {2025-03-05},
	publisher = {arXiv},
	author = {Myers, Jaden and Najafian, Keyhan and Maleki, Farhad and Ovens, Katie},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15135 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/XMPA2FXX/Myers et al. - 2024 - Modified CycleGAN for the synthesization of samples for wheat head segmentation.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/N6IPJRJ9/2402.html:text/html},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9880056},
	urldate = {2025-03-05},
	file = {IEEE Xplore Full-Text PDF\::/Users/gustavo/Zotero/storage/2LWUX4RN/stamp.html:text/html},
}

@inproceedings{lugmayr_repaint_2022,
	title = {{RePaint}: {Inpainting} using {Denoising} {Diffusion} {Probabilistic} {Models}},
	shorttitle = {{RePaint}},
	url = {https://ieeexplore.ieee.org/document/9880056},
	doi = {10.1109/CVPR52688.2022.01117},
	abstract = {Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image infor-mation. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. Re-Paint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint},
	urldate = {2025-03-05},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lugmayr, Andreas and Danelljan, Martin and Romero, Andres and Yu, Fisher and Timofte, Radu and Van Gool, Luc},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {Computer vision, Generative adversarial networks, Image and video synthesis and generation, Noise reduction, Pattern recognition, Probabilistic logic, Task analysis, Training},
	pages = {11451--11461},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/UU4CM4UK/Lugmayr et al. - 2022 - RePaint Inpainting using Denoising Diffusion Probabilistic Models.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/gustavo/Zotero/storage/G28K48AN/9880056.html:text/html},
}

@inproceedings{heusel_gans_2017,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fréchet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	urldate = {2025-03-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	year = {2017},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/JHZKS37A/Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.pdf:application/pdf},
}

@article{wang_image_2004,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	issn = {1941-0042},
	shorttitle = {Image quality assessment},
	url = {https://ieeexplore.ieee.org/document/1284395},
	doi = {10.1109/TIP.2003.819861},
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
	number = {4},
	urldate = {2025-03-05},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Data mining, Degradation, Humans, Image quality, Indexes, Layout, Quality assessment, Transform coding, Visual perception, Visual system},
	pages = {600--612},
	file = {Full Text PDF:/Users/gustavo/Zotero/storage/YBGKNSKD/Wang et al. - 2004 - Image quality assessment from error visibility to structural similarity.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/gustavo/Zotero/storage/Z7CHWXN6/1284395.html:text/html},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2025-03-05},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/EFJB5JM8/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/MMRRILQG/1406.html:text/html},
}

@misc{cohen_generative_2022,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2203.00667},
	doi = {10.48550/arXiv.2203.00667},
	abstract = {Generative Adversarial Networks (GANs) are very popular frameworks for generating high-quality data, and are immensely used in both the academia and industry in many domains. Arguably, their most substantial impact has been in the area of computer vision, where they achieve state-of-the-art image generation. This chapter gives an introduction to GANs, by discussing their principle mechanism and presenting some of their inherent problems during training and evaluation. We focus on these three issues: (1) mode collapse, (2) vanishing gradients, and (3) generation of low-quality images. We then list some architecture-variant and loss-variant GANs that remedy the above challenges. Lastly, we present two utilization examples of GANs for real-world applications: Data augmentation and face images generation.},
	urldate = {2025-03-05},
	publisher = {arXiv},
	author = {Cohen, Gilad and Giryes, Raja},
	month = mar,
	year = {2022},
	note = {arXiv:2203.00667 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/SJWVNTJ9/Cohen and Giryes - 2022 - Generative Adversarial Networks.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/UWB82WJJ/2203.html:text/html},
}

@misc{zhang_adding_2023,
	title = {Adding {Conditional} {Control} to {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.05543},
	doi = {10.48550/arXiv.2302.05543},
	abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({\textless}50k) and large ({\textgreater}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
	urldate = {2025-03-05},
	publisher = {arXiv},
	author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
	month = nov,
	year = {2023},
	note = {arXiv:2302.05543 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Human-Computer Interaction, Computer Science - Multimedia},
	annote = {Comment: Codes and Supplementary Material: https://github.com/lllyasviel/ControlNet},
	file = {Preprint PDF:/Users/gustavo/Zotero/storage/4FWZBBYB/Zhang et al. - 2023 - Adding Conditional Control to Text-to-Image Diffusion Models.pdf:application/pdf;Snapshot:/Users/gustavo/Zotero/storage/8RE9W4VT/2302.html:text/html},
}
