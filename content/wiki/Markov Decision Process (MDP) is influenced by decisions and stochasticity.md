A Markov Decison Process (MDP) is a [[A Markov process has the Markov Property|markov process]] that is partly influenced by decision and partly by [[Stochasticity|stochasticity]].

A MDP has:
- $S$ : set of states; the *state space*
- $A$ : set of actions; the *action space*
- $P_a(s,s')$ : probability a transition happens from $s$ to $s'$ due to action $a$
- $R_a(s, s')$: immediate reward/payoff/value due to state transition

#math #cs